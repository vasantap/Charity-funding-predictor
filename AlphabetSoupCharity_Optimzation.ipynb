# Import dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf
#  Import and read the charity_data.csv.
charity_data_df = pd.read_csv("Resources/charity_data.csv")
charity_data_df.head()
# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
charity_data_df.drop(columns=['EIN', 'NAME'], inplace=True)
charity_data_df
# Determine the number of unique values in each column.
charity_data_df.nunique()
# Look at APPLICATION_TYPE value counts for binning
application_counts = charity_data_df['APPLICATION_TYPE'].value_counts()
application_counts
# Choose a cutoff value and create a list of application types to be replaced
# use the variable name `application_types_to_replace
application_types_to_replace = list(application_counts[application_counts<500].index)

# Replace the application types in the dataframe with 'Other'
for application_type in application_types_to_replace:
    charity_data_df['APPLICATION_TYPE'] = charity_data_df['APPLICATION_TYPE'].replace(application_type, 'Other')

# Check to make sure binning was successful
charity_data_df['APPLICATION_TYPE'].value_counts()

# Look at CLASSIFICATION value counts for binning
classification_counts = charity_data_df['CLASSIFICATION'].value_counts()
classification_counts
# You may find it helpful to look at CLASSIFICATION value counts >1
classification_counts[classification_counts>1]
# You may find it helpful to look at CLASSIFICATION value counts >10
classification_counts[classification_counts>10]
# Choose a cutoff value and create a list of classifications to be replaced
# use the variable name `classifications_to_replace`
classifications_to_replace = list(classification_counts[classification_counts<1000].index)
  
# Replace in dataframe
for clas in classifications_to_replace:
    charity_data_df['CLASSIFICATION'] = charity_data_df['CLASSIFICATION'].replace(clas,"Other")
    
# Check to make sure binning was successful
charity_data_df['CLASSIFICATION'].value_counts()
# Convert categorical data to numeric with `pd.get_dummies`
charity_dummies_df = pd.get_dummies(charity_data_df)
charity_dummies_df.head()
# Split our preprocessed data into our features and target arrays
X = charity_dummies_df.drop('IS_SUCCESSFUL', axis='columns').values
y = charity_dummies_df['IS_SUCCESSFUL'].values


# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Get the shape of the scaled data
X_train_scaled.shape
##### Compile, Train and Evaluate the Model
# Define the model - deep neural network with one hidden layers with 80 and 30 neurons 
input_features = len(X_train[0])
print(input_features)
hidden_layer_1 = 80
hidden_layer_2 = 30

# Create a neural network model by assigning the number of input features and nodes for each layer using TensorFlow and Keras.
nn_model = tf.keras.models.Sequential()

# Add a hidden layer with 80 neurons
nn_model.add(tf.keras.layers.Dense(units=hidden_layer_1, input_dim=input_features, activation='relu'))

# Add a hidden layer with 30 neurons
nn_model.add(tf.keras.layers.Dense(units=hidden_layer_2, activation='relu'))

# Add an output layer with 1 neuron
nn_model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

# Check the structure of the model
nn_model.summary()
# Compile the model
nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# Train the model
nn_fit_model = nn_model.fit(X_train_scaled, y_train, epochs=100)
# Evaluate the model using the test data
model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")
# Export our model to HDF5 file
nn_model.save('../Resources/charity_model.h5')